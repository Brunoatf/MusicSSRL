{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Self-Supervised Learning of Music Representations for Recommendation Systems**\n",
        "\n",
        "This work explores self-supervised learning to derive music representations using the AST model and SimCLR framework.\n",
        "It aims to enhance music recommendation systems by leveraging learned embeddings. The project employs the Free Music Archive\n",
        "(FMA) dataset and addresses challenges in audio augmentation, InfoNCE loss, and self-supervised fine-tuning.\n",
        "\n",
        "\n",
        "## Group members:\n",
        "\n",
        "*   Andreas Cisi Ramos (246932)\n",
        "*   Bruno Amaral Teixeira de Freitas (246983)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ziz8IRNMws7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table of Contents <a class=\"anchor\" id=\"topo\"></a>\n",
        "\n",
        "* [Part 1: Main dependencies](#part_01).\n",
        "* [Part 2: FMA Dataset: Loading and Analysis](#part_02).\n",
        "* [Part 3: Contrastive learning: Data augmentation](#part_03).\n",
        "* [Part 4: SimCLR Model](#part_04).\n",
        "* [Part 5: Training the SimCLR Model](#part_05).\n",
        "* [Part 6: Embedding Creation and Visualization](#part_06).\n",
        "* [Part 7: Recommendation system](#part_07).\n",
        "* [Part 8: Qualitative Tests and Results](#part_08).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LKm7a2CRxvDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Main dependencies <a class=\"anchor\" id=\"part_01\"></a>\n",
        "\n",
        "Here we install dependencies, libraries, and perform the necessary imports for the project.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c1DIYifqyA4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "86UIBL_XRtcq"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchaudio transformers matplotlib torch_audiomentations sox\n",
        "!apt-get install sox libsox-dev libsox-fmt-all -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWwDQrkMRtcx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from transformers import ASTFeatureExtractor, ASTModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from IPython.display import Audio, display\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: FMA Dataset: Loading and Analysis <a class=\"anchor\" id=\"part_02\"></a>\n",
        "\n",
        "The FMA (Free Music Archive) dataset is a collection of music tracks with metadata, widely used for music classification and genre prediction tasks. In this project, we use the FMA Small version, which contains 8,000 songs, each approximately 30 seconds long, spanning multiple genres.\n",
        "\n",
        "The dataset provides:\n",
        "\n",
        "Audio files: Stored in .mp3 format.\n",
        "\n",
        "Metadata: Includes track_id and track_genres information for mapping songs to their respective genres."
      ],
      "metadata": {
        "id": "LMJbko1pIOSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOI2FL7gRtcy"
      },
      "outputs": [],
      "source": [
        "class BaseFMADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Base class for creating datasets from the FMA music dataset.\n",
        "\n",
        "    Arguments:\n",
        "    - audio_dir: Directory containing the audio files.\n",
        "    - metadata_path: Path to the metadata CSV file.\n",
        "    - file_list: List of audio file names.\n",
        "    - song_duration: Desired duration of audio in seconds (default: 10).\n",
        "\n",
        "    This class handles loading audio files, preprocessing them, and associating genres from the metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, audio_dir, metadata_path, file_list, song_duration=10):\n",
        "        \"\"\"\n",
        "        Initializes the dataset by loading metadata and setting parameters.\n",
        "\n",
        "        Arguments:\n",
        "        - audio_dir: Directory containing the audio files.\n",
        "        - metadata_path: Path to the metadata CSV file.\n",
        "        - file_list: List of audio file names.\n",
        "        - song_duration: Duration of audio clips in seconds (default: 10).\n",
        "        \"\"\"\n",
        "        self.audio_dir = audio_dir\n",
        "        self.file_list = file_list\n",
        "        self.genres = self._load_genres(metadata_path)\n",
        "        self.song_duration = song_duration\n",
        "\n",
        "    def _load_genres(self, metadata_path):\n",
        "        \"\"\"\n",
        "        Loads genre information from the metadata file.\n",
        "\n",
        "        Arguments:\n",
        "        - metadata_path: Path to the metadata CSV file.\n",
        "\n",
        "        Returns:\n",
        "        - A dictionary mapping track IDs to their genre titles.\n",
        "        \"\"\"\n",
        "        metadata = pd.read_csv(metadata_path)\n",
        "        if 'track_id' not in metadata.columns or 'track_genres' not in metadata.columns:\n",
        "            raise ValueError(\"The metadata table must contain 'track_id' and 'track_genres' columns.\")\n",
        "\n",
        "        def extract_genre_title(genre_list):\n",
        "            \"\"\"\n",
        "            Extracts the genre title from a track's genre information.\n",
        "            \"\"\"\n",
        "            try:\n",
        "                genre_data = eval(genre_list)\n",
        "                if isinstance(genre_data, list) and genre_data:\n",
        "                    return genre_data[0].get('genre_title', 'Unknown')\n",
        "                return 'Unknown'\n",
        "            except Exception:\n",
        "                return 'Unknown'\n",
        "\n",
        "        metadata['genre_title'] = metadata['track_genres'].apply(extract_genre_title)\n",
        "        return metadata.set_index('track_id')['genre_title'].to_dict()\n",
        "\n",
        "    def get_genre(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the genre of a track based on its index in the file list.\n",
        "\n",
        "        Arguments:\n",
        "        - idx: Index of the track in the file list.\n",
        "\n",
        "        Returns:\n",
        "        - The genre title as a string.\n",
        "        \"\"\"\n",
        "        track_id = int(self.file_list[idx].split('.')[0])\n",
        "        return self.genres.get(track_id, \"Unknown\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of files in the dataset.\n",
        "\n",
        "        Returns:\n",
        "        - The number of files as an integer.\n",
        "        \"\"\"\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and preprocesses an audio file by its index.\n",
        "\n",
        "        Arguments:\n",
        "        - idx: Index of the file in the file list.\n",
        "\n",
        "        Returns:\n",
        "        - A normalized and trimmed/padded waveform as a tensor.\n",
        "        \"\"\"\n",
        "        file_name = self.file_list[idx]\n",
        "        file_path = os.path.join(self.audio_dir, file_name[:3], file_name)\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample to 16 kHz if needed\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        epsilon = 1e-8\n",
        "        waveform = waveform / (waveform.abs().max() + epsilon)\n",
        "\n",
        "        # Trim or pad waveform to target length\n",
        "        target_length = 16000 * self.song_duration\n",
        "        if waveform.shape[1] < target_length:\n",
        "            padding = target_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "        else:\n",
        "            waveform = waveform[:, :target_length]\n",
        "\n",
        "        return waveform.squeeze(0)\n",
        "\n",
        "\n",
        "class TrainFMADataset(BaseFMADataset):\n",
        "    pass\n",
        "\n",
        "\n",
        "class ValFMADataset(BaseFMADataset):\n",
        "    pass\n",
        "\n",
        "\n",
        "class TestFMADataset(BaseFMADataset):\n",
        "    pass\n",
        "\n",
        "\n",
        "def prepare_datasets(audio_dir, metadata_path, num_train, num_val, num_test):\n",
        "    \"\"\"\n",
        "    Prepares training, validation, and test file lists from the dataset.\n",
        "\n",
        "    Arguments:\n",
        "    - audio_dir: Directory containing the audio files.\n",
        "    - metadata_path: Path to the metadata CSV file.\n",
        "    - num_train: Number of training samples.\n",
        "    - num_val: Number of validation samples.\n",
        "    - num_test: Number of test samples.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing three lists: train_files, val_files, test_files.\n",
        "    \"\"\"\n",
        "    train_files = []\n",
        "    val_files = []\n",
        "    test_files = []\n",
        "\n",
        "    metadata = pd.read_csv(metadata_path)\n",
        "    genres = metadata['track_id'].tolist()\n",
        "\n",
        "    count = 0\n",
        "    for i in range(156):  # Iterate through subdirectories\n",
        "        audio_dir_sub = os.path.join(audio_dir, f\"{i:03d}\")\n",
        "        if not os.path.exists(audio_dir_sub):\n",
        "            continue\n",
        "\n",
        "        for f in os.listdir(audio_dir_sub):\n",
        "            if f.endswith(\".mp3\"):\n",
        "                track_id = int(f.split('.')[0])\n",
        "                if track_id in genres:\n",
        "                    if count < num_train:\n",
        "                        train_files.append(f)\n",
        "                    elif count < num_train + num_val:\n",
        "                        val_files.append(f)\n",
        "                    elif count < num_train + num_val + num_test:\n",
        "                        test_files.append(f)\n",
        "                    else:\n",
        "                        break\n",
        "                    count += 1\n",
        "\n",
        "        if count >= num_train + num_val + num_test:\n",
        "            break\n",
        "\n",
        "    return train_files, val_files, test_files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Setup\n",
        "\n",
        "1. **Download the dataset and metadata from the links below:**\n",
        "\n",
        "fma_small : \"https://os.unil.cloud.switch.ch/fma/fma_small.zip\" (~7GB)\n",
        "\n",
        "fma_metadata: \"https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\"\n",
        "\n",
        "2. **Extract the files and organize the folders in the following structure:**\n",
        "\n",
        "```python\n",
        "fma_dataset/\n",
        "├── fma_small/\n",
        "│   ├── 000/\n",
        "│   ├── 001/\n",
        "│   └── ...\n",
        "└── fma_metadata/\n",
        "    ├── tracks.csv\n",
        "    ├── genres.csv\n",
        "    └── features.csv\n",
        "```\n",
        "\n",
        "3. **Replace the google_drive_path variable in the code with the path to the fma_dataset folder.**\n",
        "\n",
        "```python\n",
        "google_drive_path = \"/path/to/fma_dataset\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "heAao_-ga757"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS5OZpC3TJJo",
        "outputId": "a2d68805-bd45-431e-82d2-a55fa6438fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "google_drive_path = \"./drive/MyDrive/fma_dataset\"\n",
        "\n",
        "audio_dir = google_drive_path + \"/fma_small\"\n",
        "metadata_path = google_drive_path + \"/fma_metadata/raw_tracks.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and DataLoader Setup\n",
        "\n",
        "This section splits the dataset into training, validation, and test sets, then creates DataLoader instances for efficient batching and shuffling during training and evaluation.\n"
      ],
      "metadata": {
        "id": "FF3UNNwkcMdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_train = 1000\n",
        "num_val   = 200\n",
        "num_test  = 100\n",
        "\n",
        "# Prepare Datasets and Dataloaders\n",
        "train_files, val_files, test_files = prepare_datasets(audio_dir, metadata_path, num_train, num_val, num_test)\n",
        "\n",
        "train_dataset = TrainFMADataset(audio_dir, metadata_path, train_files)\n",
        "val_dataset = ValFMADataset(audio_dir, metadata_path, val_files)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "iH9KaDK90Mvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Waveforms and Mel Spectrograms for Visualization\n",
        "\n",
        "In this section, we will visualize the waveform and Mel spectrogram of the audio samples. The waveform represents the raw audio signal over time, while the Mel spectrogram displays the frequency content of the signal in a logarithmic scale, emphasizing the perceptual properties of sound. Additionally, we will play the audio for auditory inspection.\n"
      ],
      "metadata": {
        "id": "3L-pG_YUeAU9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhBJE4nvRtc0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from IPython.display import Audio\n",
        "\n",
        "def show_graphics(waveform):\n",
        "    \"\"\"\n",
        "    Function to display the waveform and Mel spectrogram of an audio file.\n",
        "\n",
        "    Arguments:\n",
        "    waveform (Tensor): The waveform of the audio file (1D tensor of audio samples).\n",
        "    \"\"\"\n",
        "\n",
        "    # Spectrogram parameters\n",
        "    sample_rate = 16000\n",
        "    n_fft = 512\n",
        "    hop_length = 128\n",
        "    n_mels = 128\n",
        "\n",
        "    #  Generate the Mel spectrogram\n",
        "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels\n",
        "    )(waveform.unsqueeze(0))\n",
        "\n",
        "    spectrogram_db = torchaudio.transforms.AmplitudeToDB()(mel_spectrogram)\n",
        "\n",
        "    spectrogram_db = spectrogram_db.squeeze(0)\n",
        "\n",
        "    num_frames = spectrogram_db.shape[1]\n",
        "    time_axis = np.linspace(\n",
        "        0,\n",
        "        (num_frames - 1) * hop_length / sample_rate,\n",
        "        num=num_frames\n",
        "    )\n",
        "\n",
        "    import math\n",
        "\n",
        "    def hz_to_mel(hz):\n",
        "        \"\"\"Converts a frequency in Hertz (Hz) to the Mel scale.\"\"\"\n",
        "        return 2595 * math.log10(1 + hz / 700)\n",
        "\n",
        "    def mel_to_hz(mel):\n",
        "        \"\"\"Converts a frequency in Mel scale to Hertz (Hz).\"\"\"\n",
        "        return 700 * (10 ** (mel / 2595) - 1)\n",
        "\n",
        "\n",
        "    # Equally spaced Mel frequencies\n",
        "    mel_min = hz_to_mel(0)\n",
        "    mel_max = hz_to_mel(sample_rate / 2)\n",
        "    mel_points = np.linspace(mel_min, mel_max, n_mels)\n",
        "\n",
        "    frequency_axis = mel_to_hz(mel_points)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
        "\n",
        "    # Plot the waveform\n",
        "    axs[0].plot(np.linspace(0, len(waveform) / sample_rate, num=len(waveform)), waveform.numpy())\n",
        "    axs[0].set_title('Waveform')\n",
        "    axs[0].set_xlabel('Time (s)')\n",
        "    axs[0].set_ylabel('Amplitude')\n",
        "\n",
        "    # Plot the Mel spectrogram\n",
        "    im = axs[1].imshow(\n",
        "        spectrogram_db.numpy(),\n",
        "        origin='lower',\n",
        "        aspect='auto',\n",
        "        extent=[time_axis.min(), time_axis.max(), frequency_axis.min(), frequency_axis.max()],\n",
        "        cmap='viridis'\n",
        "    )\n",
        "    axs[1].set_title('Mel Spectrogram')\n",
        "    axs[1].set_xlabel('Time (s)')\n",
        "    axs[1].set_ylabel('Frequency (Hz)')\n",
        "    fig.colorbar(im, ax=axs[1], format='%+2.0f dB')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Play the audio\n",
        "    display(Audio(waveform.numpy(), rate=sample_rate))\n",
        "\n",
        "show_graphics(train_dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Contrastive learning - Data augmentation<a class=\"anchor\" id=\"part_03\"></a>\n",
        "\n",
        "In this section, we apply various data augmentation techniques to augment audio samples in our dataset. For each audio sample, we apply random augmentations, which include adding noise, time stretching (without changing the pitch), or pitch shifting (without changing the duration). This process helps to increase the diversity of our dataset, improving the robustness of models trained on this data.\n",
        "\n"
      ],
      "metadata": {
        "id": "rBFG_WNAfkXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def time_stretch(audio_tensor, rate, sample_rate=16000):\n",
        "    \"\"\"\n",
        "    Function to apply time stretching to an audio waveform without altering the pitch.\n",
        "\n",
        "    Arguments:\n",
        "    audio_tensor (Tensor): The input audio waveform (1D tensor representing audio samples).\n",
        "    rate (float): The factor by which to stretch or compress the audio (1.0 means no change).\n",
        "    sample_rate (int, optional): The sample rate of the audio (default is 16000).\n",
        "\n",
        "    Returns:\n",
        "    Tensor: The time-stretched audio waveform.\n",
        "    \"\"\"\n",
        "    effects = [\n",
        "        ['tempo', f'{rate}']\n",
        "    ]\n",
        "    stretched_waveform, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
        "        audio_tensor.cpu().unsqueeze(0), sample_rate, effects)\n",
        "\n",
        "    original_length = audio_tensor.shape[-1]\n",
        "    stretched_length = stretched_waveform.shape[-1]\n",
        "\n",
        "    if stretched_length < original_length:\n",
        "        padding = original_length - stretched_length\n",
        "        stretched_waveform = torch.nn.functional.pad(stretched_waveform, (0, padding))\n",
        "    elif stretched_length > original_length:\n",
        "        stretched_waveform = stretched_waveform[:, :original_length]\n",
        "\n",
        "    return stretched_waveform.squeeze(0).to(audio_tensor.device)\n",
        "\n",
        "\n",
        "def pitch_shift(audio_tensor, sample_rate, n_steps):\n",
        "    \"\"\"\n",
        "    Function to apply pitch shifting to an audio waveform without altering the duration.\n",
        "\n",
        "    Arguments:\n",
        "    audio_tensor (Tensor): The input audio waveform (1D tensor representing audio samples).\n",
        "    sample_rate (int): The sample rate of the audio.\n",
        "    n_steps (int): The number of semitones to shift the pitch (positive for higher pitch, negative for lower).\n",
        "\n",
        "    Returns:\n",
        "    Tensor: The pitch-shifted audio waveform.\n",
        "    \"\"\"\n",
        "    n_steps_cents = n_steps * 100\n",
        "    effects = [\n",
        "        ['pitch', f'{n_steps_cents}'],\n",
        "        ['rate', f'{sample_rate}']\n",
        "    ]\n",
        "    shifted_waveform, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
        "        audio_tensor.cpu().unsqueeze(0), sample_rate, effects)\n",
        "    return shifted_waveform.squeeze(0).to(audio_tensor.device)\n",
        "\n",
        "\n",
        "def data_augmentation(audio_batch, choice=-1):\n",
        "    \"\"\"\n",
        "    Function to apply data augmentation on a batch of audio samples.\n",
        "\n",
        "    Arguments:\n",
        "    audio_batch (Tensor): A batch of audio tensors (shape: [batch_size, target_length]).\n",
        "    choice (int, optional): The augmentation choice (default is -1, which picks randomly).\n",
        "\n",
        "    Returns:\n",
        "    Tensor: The batch of augmented audio tensors.\n",
        "    \"\"\"\n",
        "    augmented_batch = []\n",
        "    for audio_tensor in audio_batch:\n",
        "        augmented_audio = data_augmentation_single(audio_tensor, choice)\n",
        "        augmented_batch.append(augmented_audio)\n",
        "    augmented_batch = torch.stack(augmented_batch)\n",
        "    return augmented_batch\n",
        "\n",
        "\n",
        "def data_augmentation_single(audio_tensor, choice=-1):\n",
        "    \"\"\"\n",
        "    Function to apply a single data augmentation to a single audio sample.\n",
        "\n",
        "    Arguments:\n",
        "    audio_tensor (Tensor): A single audio tensor (1D).\n",
        "    choice (int, optional): The augmentation choice (default is -1, which picks randomly).\n",
        "\n",
        "    Returns:\n",
        "    Tensor: The augmented audio tensor.\n",
        "    \"\"\"\n",
        "    if choice == -1:\n",
        "        choice = torch.randint(0, 3, (1,)).item()\n",
        "    if choice == 0:\n",
        "        # Add noise\n",
        "        noise = torch.randn_like(audio_tensor) * 0.05\n",
        "        return audio_tensor + noise\n",
        "    elif choice == 1:\n",
        "        # Time Stretching without changing pitch\n",
        "        rate = 1.0 + (torch.randn(1).item() * 0.2)\n",
        "        stretched_waveform = time_stretch(audio_tensor, rate)\n",
        "        return stretched_waveform\n",
        "    elif choice == 2:\n",
        "        # Pitch Shifting without changing duration\n",
        "        n_steps = torch.randint(-2, 3, (1,)).item()\n",
        "        shifted_waveform = pitch_shift(audio_tensor, 16000, n_steps)\n",
        "        return shifted_waveform\n",
        "    else:\n",
        "        return audio_tensor\n"
      ],
      "metadata": {
        "id": "1EGUmFvof22Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot and analysis of the 3 Data Augmentation Effects on the Same Audio"
      ],
      "metadata": {
        "id": "GeJmTL6chGgB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hy6KJduRtc5"
      },
      "outputs": [],
      "source": [
        "print(\"Add Noise\")\n",
        "show_graphics(data_augmentation_single(train_dataset[0], 0))\n",
        "print(\"Time Stretching\")\n",
        "show_graphics(data_augmentation_single(train_dataset[0], 1))\n",
        "print(\"Pitch Shifiting\")\n",
        "show_graphics(data_augmentation_single(train_dataset[0], 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: SimCLR Model <a class=\"anchor\" id=\"part_04\"></a>\n",
        "\n",
        "This section defines a **SimCLR model** using the AST model for feature extraction and a projection head for contrastive learning. The main components are:\n",
        "\n",
        "1. **Encoder**:\n",
        "   - If `train_from_scratch` is `True`, the AST model is initialized with random weights. Otherwise, it loads a pre-trained AST model from a specified checkpoint.\n",
        "   - The AST model is used to extract features from the input data.\n",
        "\n",
        "2. **Projection Head**:\n",
        "   - A fully connected layer that projects the output of the AST encoder into a lower-dimensional space (`projection_dim`) for contrastive learning. It consists of two linear layers with a ReLU activation in between.\n",
        "\n",
        "3. **Forward Pass**:\n",
        "   - The `forward` method processes the input through the encoder, performs global average pooling on the output, and passes it through the projection head to generate the final projected embeddings.\n",
        "\n",
        "## InfoNCE Loss\n",
        "\n",
        "The InfoNCE loss function calculates the contrastive loss between positive pairs (augmented versions of the same input) and negative pairs (embeddings from different samples in the batch). It computes the cosine similarity between all pairs, normalizes it by their L2 norms, and scales it using a temperature parameter. Masks are applied to identify positive pairs and exclude self-similarity, while negative pairs are all other embeddings. The loss is then computed by applying log softmax to the similarity matrix and averaging the log probabilities of the positive pairs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l4Q5DiTshel5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-pkCR64Rtc2"
      },
      "outputs": [],
      "source": [
        "from transformers import ASTConfig\n",
        "\n",
        "class SimCLRModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A model for SimCLR using the AST model as a feature extractor and a projection head\n",
        "    for contrastive learning. This class implements a simple architecture for self-supervised\n",
        "    learning with contrastive loss.\n",
        "\n",
        "    Args:\n",
        "        base_model_name (str, optional): Name of the pre-trained AST model to load (default is None).\n",
        "        projection_dim (int): The dimensionality of the projection head's output (default is 128).\n",
        "        train_from_scratch (bool): If True, the AST model is initialized with random weights;\n",
        "                                    otherwise, a pre-trained model is loaded (default is False).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model_name=None, projection_dim=128, train_from_scratch=False):\n",
        "        \"\"\"\n",
        "        Initializes the SimCLRModel by setting up the encoder and the projection head.\n",
        "\n",
        "        Args:\n",
        "            base_model_name (str, optional): Name of the pre-trained AST model to load (default is None).\n",
        "            projection_dim (int): The dimensionality of the projection head's output (default is 128).\n",
        "            train_from_scratch (bool): Flag to initialize the AST model with random weights if True,\n",
        "                                        or load a pre-trained model if False (default is False).\n",
        "        \"\"\"\n",
        "        super(SimCLRModel, self).__init__()\n",
        "\n",
        "        if train_from_scratch:\n",
        "            self.encoder = ASTModel(ASTConfig())  # Default AST configuration\n",
        "        else:\n",
        "            self.encoder = ASTModel.from_pretrained(base_model_name)\n",
        "\n",
        "        # Projection head for SimCLR (maps the encoder output to the projection space)\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the encoder and projection head.\n",
        "\n",
        "        Args: x (dict): The input tensor in the form of a dictionary to be passed to the encoder.\n",
        "              It must include the necessary inputs for the AST model (e.g., audio data).\n",
        "\n",
        "        Returns: torch.Tensor: The projected embeddings after applying the projection head to the pooled output.\n",
        "        \"\"\"\n",
        "        outputs = self.encoder(**x)\n",
        "        pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
        "        projections = self.projection_head(pooled_output)\n",
        "\n",
        "        return projections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv7WIB8vRtc4"
      },
      "outputs": [],
      "source": [
        "def info_nce_loss(batch_embeddings, temperature=0.07):\n",
        "    \"\"\"\n",
        "    Computes the InfoNCE loss considering the entire batch.\n",
        "\n",
        "    Args:\n",
        "        batch_embeddings (torch.Tensor): Concatenated embeddings from augmented versions.\n",
        "                                         Shape: [2 * batch_size, embedding_dim].\n",
        "        temperature (float): Temperature factor to scale similarity.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: InfoNCE loss value.\n",
        "    \"\"\"\n",
        "    # Cosine similarity between all pairs in the batch\n",
        "    cos_sim = torch.mm(batch_embeddings, batch_embeddings.T)  # Normalized dot product\n",
        "    cos_sim /= torch.norm(batch_embeddings, dim=1).unsqueeze(1)  # L2 normalization\n",
        "\n",
        "    # Temperature scaling\n",
        "    cos_sim = cos_sim / temperature\n",
        "\n",
        "    # Mask to avoid self-similarity\n",
        "    batch_size = batch_embeddings.size(0) // 2\n",
        "    self_mask = torch.eye(2 * batch_size, device=batch_embeddings.device).bool()\n",
        "    cos_sim.masked_fill_(self_mask, -float('inf'))\n",
        "\n",
        "    # Mask for positive pairs\n",
        "    pos_mask = torch.zeros_like(cos_sim, dtype=torch.bool, device=batch_embeddings.device)\n",
        "    for i in range(batch_size):\n",
        "        pos_mask[i, i + batch_size] = True\n",
        "        pos_mask[i + batch_size, i] = True\n",
        "\n",
        "    # InfoNCE loss\n",
        "    log_prob = F.log_softmax(cos_sim, dim=-1)\n",
        "    loss = -log_prob[pos_mask].mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Training SimCLR Model <a class=\"anchor\" id=\"part_05\"></a>\n",
        "\n",
        "The `train_simclr_model` function is designed to train a SimCLR model using the InfoNCE loss. It allows flexibility to train the model from scratch or fine-tune a pre-trained model.\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. **Feature Extractor**:\n",
        "   An AST feature extractor is loaded to preprocess augmented audio data into suitable input tensors.\n",
        "\n",
        "2. **Training Loop**:\n",
        "   - The model is trained over the specified number of epochs.\n",
        "   - For each batch in the training dataset:\n",
        "     - Data augmentation is applied to generate two augmented versions of the audio input.\n",
        "     - These augmented samples are processed using the feature extractor.\n",
        "     - The model generates embeddings for both versions.\n",
        "     - The embeddings are concatenated, and the InfoNCE loss is calculated.\n",
        "\n",
        "3. **Validation**:\n",
        "   - After each epoch, the model's performance is evaluated on the validation dataset.\n",
        "   - Validation loss is computed similarly to the training process.\n",
        "\n",
        "4. **Model Saving**:\n",
        "   After training is completed, the model's weights are saved to the specified file.\n",
        "\n",
        "## Outputs\n",
        "The function returns a dictionary containing:\n",
        "- **`train_losses`**: List of training losses for each epoch.\n",
        "- **`val_losses`**: List of validation losses for each epoch.\n",
        "\n",
        "## Example Usage\n",
        "```python\n",
        "results = train_simclr_model(\n",
        "    epochs=10,\n",
        "    lr=3e-5,\n",
        "    save_model_name=\"simclr_trained_model.pth\",\n",
        "    train_from_scratch=True\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "58LEEHv3Fxbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_simclr_model(epochs, lr, save_model_name = \"simclr_trained_model.pth\", train_from_scratch = True):\n",
        "    \"\"\"\n",
        "    Trains the SimCLR model using the InfoNCE loss.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of training epochs.\n",
        "        lr (float): Learning rate for the optimizer.\n",
        "        save_model_name (str): File name for saving the trained model.\n",
        "        train_from_scratch (bool): Whether to train the model from scratch or use a pre-trained base.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing training and validation losses.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device selected:\", device)\n",
        "\n",
        "    # Initialize model\n",
        "    model = SimCLRModel(\n",
        "        base_model_name=\"MIT/ast-finetuned-audioset-10-10-0.4593\",\n",
        "        train_from_scratch= train_from_scratch\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            audio = batch.to(device)\n",
        "\n",
        "            # Data augmentation for the entire batch\n",
        "            augmented_1 = data_augmentation(audio)\n",
        "            augmented_2 = data_augmentation(audio)\n",
        "\n",
        "            # Convert to NumPy and apply feature extractor\n",
        "            inputs_1 = feature_extractor(\n",
        "                list(augmented_1.cpu().numpy()), sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
        "            )\n",
        "            inputs_2 = feature_extractor(\n",
        "                list(augmented_2.cpu().numpy()), sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
        "            )\n",
        "            inputs_1 = {k: v.to(device) for k, v in inputs_1.items()}\n",
        "            inputs_2 = {k: v.to(device) for k, v in inputs_2.items()}\n",
        "\n",
        "            # Forward pass for both augmented versions\n",
        "            projections_1 = model(inputs_1)\n",
        "            projections_2 = model(inputs_2)\n",
        "\n",
        "            # Concatenate projections of the two versions\n",
        "            batch_embeddings = torch.cat([projections_1, projections_2], dim=0)\n",
        "\n",
        "            # Calculate InfoNCE loss\n",
        "            loss = info_nce_loss(batch_embeddings)\n",
        "\n",
        "            # Optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                audio = batch.to(device)\n",
        "                augmented = data_augmentation(audio)\n",
        "                inputs = feature_extractor(\n",
        "                    list(augmented.cpu().numpy()), sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
        "                )\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                # Forward pass for validation\n",
        "                projections = model(inputs)\n",
        "                concat_projections = torch.cat([projections, projections], dim=0)\n",
        "                val_epoch_loss += info_nce_loss(concat_projections).item()\n",
        "\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_epoch_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), save_model_name)\n",
        "    print(f\"Model saved as {save_model_name}\")\n",
        "\n",
        "    return {\"train_losses\": train_losses, \"val_losses\": val_losses}"
      ],
      "metadata": {
        "id": "V8E3WznREa7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "cfjkuYn-Gcjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to train the model\n",
        "results = train_simclr_model(\n",
        "    epochs=10,\n",
        "    lr=3e-5,\n",
        "    save_model_name=\"ssimclr_trained_model.pth\",\n",
        "    train_from_scratch=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd11DDXkFfk0",
        "outputId": "397a600e-dc09-4392-e876-4dcee34c8ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device selected: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing epoch Losses\n",
        "\n",
        "The `plot_losses` function visualizes the training and validation losses across epochs, allowing users to monitor and compare the performance of their model during training.\n",
        "\n",
        "## Visualization Purpose\n",
        "- **Compare Training vs. Validation Loss**:\n",
        "  The plot provides insights into how well the model is fitting the training data and how it generalizes to validation data.\n",
        "  \n",
        "- **Overfitting/Underfitting Detection**:\n",
        "  By analyzing the trends, users can detect overfitting (validation loss higher than training loss) or underfitting (both losses remaining high).\n",
        "\n"
      ],
      "metadata": {
        "id": "GKcOFfAiHeIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "  \"\"\"\n",
        "    Plots the training and validation losses over the epochs.\n",
        "\n",
        "    Args:\n",
        "        train_losses (list): List of training losses for each epoch.\n",
        "        val_losses (list): List of validation losses for each epoch.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "\n",
        "  epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "  plt.plot(epochs, train_losses, label=\"Train\", marker='o')\n",
        "  plt.plot(epochs, val_losses, label=\"Validation\", marker='o', linestyle='--')\n",
        "\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "\n",
        "  plt.title(\"Train and Validation losses over the epochs\")\n",
        "\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "uJDbcRzEoGK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the graph"
      ],
      "metadata": {
        "id": "bzcoa7KDH75L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(results[\"train_losses\"], results[\"val_losses\"])"
      ],
      "metadata": {
        "id": "YU6ZZFjzHKTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6: Embedding Creation and Visualization <a class=\"anchor\" id=\"part_06\"></a>\n",
        "\n",
        "This code compares the embeddings generated by the fine-tuned and original SimCLR models using a validation dataset. The fine-tuned model is loaded with previously trained weights, while the original model uses the default, non-fine-tuned weights.\n",
        "\n",
        "The extract_embeddings function processes the audio data batch by batch, producing embeddings_fine_tuned from the fine-tuned model and embeddings_original from the original model. These embeddings can be used for further analysis, such as evaluating training quality or visualizing differences between the two models.\n",
        "\n"
      ],
      "metadata": {
        "id": "mjp45zzkdCv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio(audio_tensor, target_length=80000):\n",
        "    \"\"\"\n",
        "    Preprocesses an audio tensor by converting it to mono, normalizing it, and adjusting its length.\n",
        "\n",
        "    Args:\n",
        "        audio_tensor (torch.Tensor): Input audio tensor.\n",
        "        target_length (int): Desired length of the output audio tensor.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Preprocessed audio tensor.\n",
        "    \"\"\"\n",
        "    if audio_tensor.dim() > 1:\n",
        "        audio_tensor = torch.mean(audio_tensor, dim=0, keepdim=True)  # Convert to mono if multi-channel\n",
        "\n",
        "    audio_tensor = audio_tensor.squeeze(0)  # Ensure correct dimensionality\n",
        "    epsilon = 1e-8\n",
        "    audio_tensor = audio_tensor / (audio_tensor.abs().max() + epsilon)  # Normalize by maximum amplitude\n",
        "\n",
        "    # Adjust the length of the audio tensor\n",
        "    if audio_tensor.shape[-1] < target_length:\n",
        "        padding = target_length - audio_tensor.shape[-1]\n",
        "        audio_tensor = torch.nn.functional.pad(audio_tensor, (0, padding))  # Pad if shorter\n",
        "    else:\n",
        "        audio_tensor = audio_tensor[:target_length]  # Truncate if longer\n",
        "\n",
        "    return audio_tensor\n",
        "\n",
        "def extract_embeddings(model, loader, feature_extractor):\n",
        "    \"\"\"\n",
        "    Extracts embeddings from audio data using a given model and feature extractor.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Pre-trained SimCLR model.\n",
        "        loader (DataLoader): DataLoader with audio batches.\n",
        "        feature_extractor (ASTFeatureExtractor): Pre-trained feature extractor.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Numpy array containing extracted embeddings.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            audio = batch.to(device)\n",
        "            # Preprocess each audio sample in the batch\n",
        "            processed_audio = [preprocess_audio(a) for a in audio]\n",
        "            audio_np = torch.stack(processed_audio).cpu().numpy()  # Stack into a single batch and convert to NumPy\n",
        "            inputs = feature_extractor(\n",
        "                list(audio_np), sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to device\n",
        "            outputs = model.encoder(**inputs)  # Forward pass through the model\n",
        "            pooled_output = outputs.last_hidden_state.mean(dim=1)  # Perform global average pooling\n",
        "            embeddings.append(pooled_output.cpu().numpy())  # Append embeddings to the list\n",
        "    embeddings = np.concatenate(embeddings, axis=0)  # Concatenate all embeddings into a single array\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "hVlibQ3FbxLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you have a saved model and want to use its pre-trained weights, run this cell.**\n"
      ],
      "metadata": {
        "id": "GwyQyvyPi1mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "\n",
        "#Complete the path to your saved model\n",
        "model_path = './drive/MyDrive/simclr_pretrained_model.pth'\n",
        "\n",
        "# Instantiating the model (use the correct name of the pre-trained AST model if needed)\n",
        "fine_tuned_model = SimCLRModel(\"MIT/ast-finetuned-audioset-10-10-0.4593\")  # If you used a specific name for the model\n",
        "\n",
        "try:\n",
        "    model_weights = torch.load(model_path)\n",
        "    print(\"File loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading the file: {e}\")\n",
        "\n",
        "# Loading the saved weights\n",
        "fine_tuned_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Setting the model to evaluation mode (if you'll be using it for inference)\n",
        "fine_tuned_model.eval()\n",
        "fine_tuned_model = fine_tuned_model.to(device)\n",
        "\n",
        "\n",
        "# Load the original model\n",
        "original_model = SimCLRModel(\"MIT/ast-finetuned-audioset-10-10-0.4593\", train_from_scratch=True)\n",
        "original_model = original_model.to(device)\n",
        "original_model.eval()\n",
        "\n",
        "# Extract embeddings for the fine-tuned and original models\n",
        "fine_tuned_embeddings = extract_embeddings(fine_tuned_model, val_loader, feature_extractor)\n",
        "original_embeddings = extract_embeddings(original_model, val_loader, feature_extractor)\n"
      ],
      "metadata": {
        "id": "ea4XTHw1Jbvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you want to use the trained model without uploading, run this cell.**"
      ],
      "metadata": {
        "id": "ZMjwAg0Fjq3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"simclr_trained_model.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "feature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = SimCLRModel(\"MIT/ast-finetuned-audioset-10-10-0.4593\", train_from_scratch=False)\n",
        "fine_tuned_model.load_state_dict(torch.load(model_name))\n",
        "fine_tuned_model = fine_tuned_model.to(device)\n",
        "fine_tuned_model.eval()\n",
        "\n",
        "# Load the original model\n",
        "original_model = SimCLRModel(\"MIT/ast-finetuned-audioset-10-10-0.4593\", train_from_scratch=True)\n",
        "original_model = original_model.to(device)\n",
        "original_model.eval()\n",
        "\n",
        "# Extract embeddings for the fine-tuned and original models\n",
        "fine_tuned_embeddings = extract_embeddings(fine_tuned_model, val_loader, feature_extractor)\n",
        "original_embeddings = extract_embeddings(original_model, val_loader, feature_extractor)"
      ],
      "metadata": {
        "id": "9Up0jQYfcw4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Visualization\n",
        "\n",
        "This code visualizes the embeddings generated by both fine-tuned and original SimCLR models using t-SNE for dimensionality reduction. The embeddings from both models are combined, standardized, and then reduced to 2D for easier visualization.\n",
        "\n",
        "Each data point (embedding) is colored based on its genre, allowing for a clearer comparison between the embeddings from the fine-tuned and original models.\n",
        "\n",
        "The code also includes a function to map each genre to a unique color, and then it plots the embeddings for both models in side-by-side subplots, with a legend indicating the genres. The final plot is displayed with adjusted layout for clarity."
      ],
      "metadata": {
        "id": "3Z3zVmDHeisP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def apply_tsne(fine_tuned_embeddings, original_embeddings, perplexity=10, random_state=42):\n",
        "    \"\"\"\n",
        "    Applies t-SNE for dimensionality reduction on the combined embeddings of both models.\n",
        "\n",
        "    Parameters:\n",
        "    - fine_tuned_embeddings: Embeddings generated by the fine-tuned model.\n",
        "    - original_embeddings: Embeddings generated by the original model.\n",
        "    - perplexity: The perplexity parameter for t-SNE.\n",
        "    - random_state: Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - embeddings_fine_tuned_2d: 2D embeddings for the fine-tuned model.\n",
        "    - embeddings_original_2d: 2D embeddings for the original model.\n",
        "    \"\"\"\n",
        "    combined_embeddings = np.concatenate([fine_tuned_embeddings, original_embeddings], axis=0)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    combined_embeddings = scaler.fit_transform(combined_embeddings)\n",
        "\n",
        "    # Apply t-SNE for dimensionality reduction (from high-dimensional to 2D)\n",
        "    tsne_2d = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n",
        "    embeddings_2d = tsne_2d.fit_transform(combined_embeddings)\n",
        "\n",
        "    # Split the reduced 2D embeddings back into fine-tuned and original sets\n",
        "    embeddings_fine_tuned_2d = embeddings_2d[:len(fine_tuned_embeddings)]\n",
        "    embeddings_original_2d = embeddings_2d[len(fine_tuned_embeddings):]\n",
        "\n",
        "    return embeddings_fine_tuned_2d, embeddings_original_2d\n",
        "\n",
        "\n",
        "def plot_embeddings(fine_tuned_embeddings_2d, original_embeddings_2d, fine_tuned_genres, original_genres):\n",
        "    \"\"\"\n",
        "    Plots the 2D embeddings of both fine-tuned and original models with colors based on genres.\n",
        "\n",
        "    Parameters:\n",
        "    - fine_tuned_embeddings_2d: 2D embeddings for the fine-tuned model.\n",
        "    - original_embeddings_2d: 2D embeddings for the original model.\n",
        "    - fine_tuned_genres: List of genres for the fine-tuned model.\n",
        "    - original_genres: List of genres for the original model.\n",
        "    \"\"\"\n",
        "    # Map genres to unique colors for visualization\n",
        "    all_genres = fine_tuned_genres + original_genres\n",
        "    unique_genres = sorted(set(all_genres))\n",
        "    genre_to_color = {genre: i for i, genre in enumerate(unique_genres)}\n",
        "    num_genres = len(unique_genres)\n",
        "    colors = plt.cm.get_cmap('tab10', num_genres)\n",
        "\n",
        "    # Function to map genres to corresponding colors\n",
        "    def get_colors(genres):\n",
        "        return [colors(genre_to_color[genre]) for genre in genres]\n",
        "\n",
        "    fine_tuned_colors = get_colors(fine_tuned_genres)\n",
        "    original_colors = get_colors(original_genres)\n",
        "\n",
        "    plt.figure(figsize=(16, 8))  # Adjust figure size\n",
        "\n",
        "    # Plot fine-tuned embeddings\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for genre in unique_genres:\n",
        "        genre_indices = [i for i, g in enumerate(fine_tuned_genres) if g == genre]\n",
        "        plt.scatter(\n",
        "            fine_tuned_embeddings_2d[genre_indices, 0],\n",
        "            fine_tuned_embeddings_2d[genre_indices, 1],\n",
        "            label=genre,\n",
        "            alpha=0.7\n",
        "        )\n",
        "    plt.title('Trained Embeddings')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "\n",
        "    # Plot original embeddings\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for genre in unique_genres:\n",
        "        genre_indices = [i for i, g in enumerate(original_genres) if g == genre]\n",
        "        plt.scatter(\n",
        "            original_embeddings_2d[genre_indices, 0],\n",
        "            original_embeddings_2d[genre_indices, 1],\n",
        "            label=genre,\n",
        "            alpha=0.7\n",
        "        )\n",
        "    plt.title('Untrained Embeddings')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "    plt.legend(title='Genres', bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.)  # Adjust legend position\n",
        "\n",
        "    # Final adjustments for better layout and show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ycL2YVV1GiH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_embeddings_2d, original_embeddings_2d = apply_tsne(fine_tuned_embeddings, original_embeddings)\n",
        "\n",
        "fine_tuned_genres = [val_dataset.get_genre(idx) for idx in range(len(fine_tuned_embeddings))]\n",
        "original_genres = [val_dataset.get_genre(idx) for idx in range(len(original_embeddings))]\n",
        "\n",
        "plot_embeddings(fine_tuned_embeddings_2d, original_embeddings_2d, fine_tuned_genres, original_genres)"
      ],
      "metadata": {
        "id": "NePxrc21fE8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7: Recommendation system <a class=\"anchor\" id=\"part_07\"></a>\n",
        "\n",
        "This section of the code implements a music recommendation system by comparing embeddings generated from fine-tuned and original models. The `recommend_song` function calculates similarities between a query embedding and a dataset of embeddings using either cosine similarity or Euclidean distance. It returns the top closest matches along with their similarity or distance scores. The `display_audio` function allows the playback of audio samples directly in the notebook for user interaction, enhancing the visualization of results.\n",
        "\n",
        "To demonstrate the system, an audio sample is selected from the validation dataset, and its embedding is computed using the fine-tuned model. Recommendations are then generated based on both fine-tuned and original embeddings, showing how model tuning affects the similarity scores and quality of suggestions. Results for each recommendation include the Music ID, Genre, and similarity/distance score, along with the audio playback of both the queried song and recommended tracks, allowing for a direct and intuitive comparison.\n"
      ],
      "metadata": {
        "id": "w1mNBONuJpsU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGaijRYARtc-"
      },
      "outputs": [],
      "source": [
        "def recommend_song(query_embedding, embeddings, top_k=1, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Recommend songs based on the closest embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - query_embedding: The embedding of the query song.\n",
        "    - embeddings: The embeddings of the dataset.\n",
        "    - top_k: Number of recommendations to return.\n",
        "    - metric: Similarity metric ('euclidean' or 'cosine').\n",
        "\n",
        "    Returns:\n",
        "    - List of tuples (index, distance/similarity) for the top_k nearest songs.\n",
        "    \"\"\"\n",
        "    if metric == 'cosine':\n",
        "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
        "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "        distances = np.dot(embeddings, query_embedding.T)\n",
        "        nearest_indices = np.argsort(-distances)[:top_k]\n",
        "        return [(index, distances[index]) for index in nearest_indices]\n",
        "    elif metric == 'euclidean':\n",
        "        distances = np.linalg.norm(embeddings - query_embedding, axis=1)\n",
        "        nearest_indices = np.argsort(distances)[:top_k]\n",
        "        return [(index, distances[index]) for index in nearest_indices]\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def display_audio(audio_tensor, sample_rate=16000):\n",
        "    \"\"\"\n",
        "    Display an audio player in the notebook.\n",
        "\n",
        "    Parameters:\n",
        "    - audio_tensor: The waveform tensor to play.\n",
        "    - sample_rate: Sampling rate of the audio.\n",
        "    \"\"\"\n",
        "    if torch.is_tensor(audio_tensor):\n",
        "        audio_np = audio_tensor.cpu().numpy()\n",
        "    else:\n",
        "        audio_np = audio_tensor\n",
        "    display(Audio(audio_np, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommendation_systems(\n",
        "    test_index, top_k_fine_tuned=2, top_k_original=1\n",
        "):\n",
        "    \"\"\"\n",
        "    Tests and compares recommendation systems using fine-tuned and original embeddings by leveraging a K-Nearest Neighbors (KNN) approach to identify and rank the most similar audio tracks.\n",
        "\n",
        "    Args:\n",
        "        test_index (int): Index of the test audio in the validation dataset.\n",
        "        top_k_fine_tuned (int): Number of recommendations to return using fine-tuned embeddings.\n",
        "        top_k_original (int): Number of recommendations to return using original embeddings.\n",
        "    \"\"\"\n",
        "    # Select the test audio\n",
        "    test_audio = val_dataset[test_index]\n",
        "\n",
        "    # Extract embedding for the test audio\n",
        "    test_embedding = extract_embeddings(\n",
        "        fine_tuned_model,\n",
        "        DataLoader([test_audio], batch_size=1),\n",
        "        feature_extractor\n",
        "    )\n",
        "\n",
        "    # Recommendations using fine-tuned embeddings\n",
        "    recommendations_fine_tuned = recommend_song(\n",
        "        test_embedding[0], fine_tuned_embeddings, top_k=top_k_fine_tuned\n",
        "    )\n",
        "\n",
        "    # Recommendations using original embeddings\n",
        "    recommendations_original = recommend_song(\n",
        "        test_embedding[0], original_embeddings, top_k=top_k_original\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    print(\"Queried Audio:\")\n",
        "    print(f\"Music ID: {test_index}, Genre: {val_dataset.get_genre(test_index)}\")\n",
        "    display_audio(test_audio.squeeze(0))\n",
        "\n",
        "    print(\"\\nRecommendations (Fine-Tuned Embeddings):\")\n",
        "    for rec_idx, distance in recommendations_fine_tuned:\n",
        "        if rec_idx == test_index:\n",
        "            continue\n",
        "        print(f\"Music ID: {rec_idx}, Genre: {val_dataset.get_genre(rec_idx)}, Cosine Similarity: {distance:.4f}\")\n",
        "        rec_audio = val_dataset[rec_idx]\n",
        "        display_audio(rec_audio)\n",
        "\n",
        "    print(\"\\nRecommendations (Original Embeddings):\")\n",
        "    for rec_idx, distance in recommendations_original:\n",
        "        print(f\"Music ID: {rec_idx}, Genre: {val_dataset.get_genre(rec_idx)}, Cosine Similarity: {distance:.4f}\")\n",
        "        rec_audio = val_dataset[rec_idx]\n",
        "        display_audio(rec_audio)"
      ],
      "metadata": {
        "id": "QwK22eG0N96S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test our recommendation system with a music index between 0 and len(val_dataset) - 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BirCtTadTwbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recommendation_systems(test_index= 0)"
      ],
      "metadata": {
        "id": "cxIQGMsNOOuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 8: Qualitative Tests and Results <a class=\"anchor\" id=\"part_08\"></a>\n",
        "\n",
        "In this section, we aim to evaluate the performance of our music recommendation system by conducting **50 tests** ( or len(val_dataset) ) using random samples from the validation dataset. Each recommendation is assessed and categorized into one of the following levels of satisfaction:\n",
        "\n",
        "1. **Unsatisfactory**: The recommendations do not align with the queried audio or genre.\n",
        "2. **Partially Satisfactory**: The recommendations partially match the queried audio or genre but lack precision.\n",
        "3. **Satisfactory**: The recommendations closely match the queried audio or genre.\n",
        "\n",
        "The evaluation is conducted manually, with human judgment serving as the benchmark for categorization. This approach ensures qualitative insights into the system's performance.\n"
      ],
      "metadata": {
        "id": "oC3rfMSgTK1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "num_val = len(val_dataset)\n",
        "max_iterations = min(50, num_val)\n",
        "random_indices = random.sample(range(num_val), max_iterations)\n",
        "\n",
        "for i, test_index in enumerate(random_indices):\n",
        "    print(f\"\\n----------------- Test {i + 1} ----------------------\")\n",
        "    recommendation_systems(test_index=test_index)"
      ],
      "metadata": {
        "id": "8KeZ9yRaTRw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "Based on the qualitative analysis conducted by humans on the **50 evaluated music tracks**, we obtained the following final results:\n",
        "\n",
        "- **Satisfactory**: 23 tracks (46% of the total)\n",
        "- **Partially Satisfactory**: 15 tracks (30% of the total)\n",
        "- **Unsatisfactory**: 17 tracks (34% of the total)\n",
        "\n",
        "Comparing with the original model without training, we obtained the following results:\n",
        "\n",
        "- Satisfactory: 5 tracks (10% of the total)\n",
        "- Partially Satisfactory: 10 tracks (20% of the total)\n",
        "- Unsatisfactory: 35 tracks (70% of the total)\n",
        "\n",
        "These results clearly demonstrate that the model has made significant progress. Although the analysis remains subjective and there is still room for improvement, the performance is notably better than what we would expect from a model trained without labeled data. The increase in \"Satisfactory\" and \"Partially Satisfactory\" tracks, coupled with the decrease in \"Unsatisfactory\" tracks, indicates that the model is on the right learning path. This suggests that, with further fine-tuning and refinement, the model could continue to improve and reach even better performance."
      ],
      "metadata": {
        "id": "vR2H5w6NWawV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}